---
###[ Ansible Configuration Options ]#############################################################
# Do not log sensitive information / tokens, etc.  Maybe be handy to set to "false" during
# troubleshooting if you need to see ansible vault secret values.
no_log_secrets: true

install:
  ###[ Linux OS Configuration ]####################################################################
  os:
    non_root_user:                            # Some specifics about the non-root user
      name: "kube"
      shell: "/bin/bash"
      groups: "sudo"

    allow_passwordless_sudo: true             # Allow non-root use to passwordless sudo

    remove_snapd:                             # Remove Snapd Demon, we don't need it.
      remove_it: true
      packages:
        - snapd

    # Send all logs to a centralized logging server. This will be the 1st server defined
    # within the "k3s_control" group. All other cluster members will send logs to this system.
    # All logs from k3s hosts will be stored under: /var/log/central/<hostname>.log
    
    central_logging:                          
      log_path: "/var/log/central"            # Directory on central server to hold logs

      packages:
        - lnav                                # Handy program to watch logs in real-time

  ###[ Traefik Installation Settings ]#############################################################
  traefik:
    # List of Domain Names for LetsEncrypt Certificates
    domains: "{{k3s.traefik.le_domains}}"
      
    # Traefik Dashboard
    dashboard:
      create_route: true                      # Create Ingress Router to make accessible 
      enable_https: true                      # Require HTTPS to access dashboard
      enable_basic_auth: true                 # Require Authentication to access dashboard
      
      # Encoded users and passwords for basic authentication
      allowed_users: "{{k3s.traefik.dashboard_users}}"    

  ###[ Kube-VIP Installation Settings ]############################################################
  # Kube-VIP provides HA Load Balancer for the API service and optionally LoadBalancer services
  # It can optionally also provide Load Balancer services a pool of address to use instead of
  # using MetalLB.  When enabled this will disable k3s built-in Klipper Load Balancer.
  kube_vip:
    enabled: true

    # Select release to use: https://github.com/kube-vip/kube-vip/releases
    install_version: "v0.4.2"

    # Define address for HA VIP (Kubenetes API will use this), you can set kubectl config to point here
    endpoint_address: "192.168.10.239"

    # To use Kube-VIP Cloud Provider as LoadBalancer instead of MetalLB
    lb:
      enabled: true
      # to use a range:  range-global: 92.168.1.220-192.168.1.230
      # to use a CIDR:   cidr-global: 192.168.0.220/29
      ip_range: "range-global: 192.168.10.240-192.168.10.253"

  ###[ MetalLB Installation Settings ]#############################################################
  # When enabled this will disable k3s built-in Klipper Load Balancer and enable MetalLB instead.
  # This can provide Load Balancer services a pool of addresses to use
  metallb:
    enabled: false

    # Select release to use:  https://github.com/metallb/metallb/releases
    # Alternate: https://metallb.universe.tf/installation/
    install_version: "v0.12.1"

    # Ranges of IP Addresses MetalLB can handout for LoadBalancer Services
    ip_ranges: 
      - "192.168.10.240-192.168.10.253"

  k3s:
    # Packages required for Ansible to issue Kubernetes and commands
    packages:
      - python3-pip
    packages_pip:
      - openshift
      - pyyaml
      - kubernetes

    # CLI options passed directly to install script "as-is":
    cli_options:
      # Do not start service after installation as it will have issues with ZFS
      - "INSTALL_K3S_SKIP_START=true"
      
      # This is to pin a specific version of k3s for initial installation
      - "INSTALL_K3S_VERSION=v1.23.4+k3s1"
      
      # Select installation channel to use (stable, latest, testing)
      # - "INSTALL_K3S_CHANNEL=latest"

    k3s_init_options: >-
      {% if groups['k3s_control'] | length > 1 -%}
      {% if ansible_host == hostvars[groups['k3s_control'][0]]['ansible_host'] | default(groups['k3s_control'][0]) -%}
      --cluster-init
      {%- else %}
      --server https://{{ hostvars[groups['k3s_control'][0]]['ansible_host'] | default(groups['k3s_control'][0]) }}:6443
      {%- endif %}
      --token {{ K3S_TOKEN|default('top_secret') }}
      {%- endif %}

    # This becomes the "INSTALL_K3S_EXEC=" CLI parameter
    k3s_exec_options:
      - "{{k3s_cli_var|default('')}}"         # Options set in inventory or hosts vars

    # When containerd for ZFS is enabled, this will be added to k3s_exec_options
    k3s_containerd_options:
      - "--container-runtime-endpoint unix:///run/containerd/containerd.sock"

    # When "disable_local_path_as_default_storage_class = true" this will be added to k3s_exec_options
    cli_disable_storage_options:              
      - "--disable local-storage"   

    # When "install.metallb.enabled == true" or "install.kube_vip.lb.enabled == true"
    # this will be added to k3s_exec_options
    cli_disable_loadbalancer_options:        
      - "--disable servicelb"

    # Location for K3s installation script
    url: "https://get.k3s.io"

    # If enabled, will fail ansible deployment when "kubectl get node" returns "No resources found"
    confirm_running: true
  
    # Create a handy alias for kubectl  ($ k get all -A)
    alias:
      enabled: true
      value: "k"
      command: "kubectl"

  ###[ Containerd Installation Settings ]##########################################################
  containerd:
    enabled: true

    packages:
      - containerd
      - containernetworking-plugins
      - iptables
    
    # Location generate config.toml file
    config_path: "/etc/containerd"

    # Location to place flannel.conflist
    flannel_conflist_path: "/etc/cni/net.d"

    # Hint to find the ZFS pool & dataset to create containerd mount point
    zfs:
      # detect_uuid will determine the UUID name used for the dataset name and include it.
      # ZFS on Root guide uses a random set of characters (UUID) in the naming convention
      # such as:  "rpool/ROOT/ubuntu_3wgs2q" where "3wgs2q" is the UUID to detect.
      
      # You can set to false and set your own or set uuid to empty string.

      # End result would be a dataset name such as:  rpool/ROOT/ubuntu_3wgs2q/var/lib/containerd
      detect_uuid: false
      pool: "rpool"
      dataset_prefix: "containerd"            # "ROOT/ubuntu"
      uuid: ""                                # "_"
      dataset_postfix: ""                     # "/var/lib/containerd"
  
  ###[ Cert Manager Installation Settings ]########################################################
  cert_manager:
    # Select release to use:  https://github.com/cert-manager/cert-manager/releases
    install_version: "v1.7.1"

  ###[ Democratic CSI Installation Settings ]######################################################
  democratic_csi:

    truenas:
      # See file "truenas_api_secrets.yml" to set secrets (and be sure to use ansible vault to encrypt!)
      http_connection:
        protocol: "https"
        port: 443
        allow_insecure: false
        host: "{{democratic_csi.http_hostname}}"
        api_key: "{{democratic_csi.http_api_key}}"

      ssh_connection:
        host: "{{democratic_csi.ssh_hostname}}"
        port: 22
        user: "{{democratic_csi.ssh_username}}"
        
        # Must use a password or private key
        #password: "{{democratic_csi.ssh_password}}"
        private_key: "{{democratic_csi.ssh_private_key}}"

      iscsi_connection:
        host: "{{democratic_csi.iscsi_hostname}}"
        port: 3260
        #interface: ""                 # leave empty to omit usage of -I with iscsiadm

      nfs_connection:
        host: "{{democratic_csi.nfs_hostname}}"

    ###[ Democratic CSI iSCSI Settings ]###########################################################
    # Available: https://github.com/democratic-csi/democratic-csi/tree/master/examples
    iscsi:
      install_this: true            # Install the iSCSI provisioner

      provisioner: "freenas-iscsi"
      namespace: "democratic-csi"
      release: "truenas-iscsi"

      packages:
        - open-iscsi
        - lsscsi
        - sg3-utils
        - multipath-tools
        - scsitools

      storage_class:
        default_class: false
        reclaim_policy: "Delete"    # "Retain", "Recycle" or "Delete"
        volume_expansion: true

      zfs:
        # Assumes pool named "main", dataset named "k8s", child dataset "iscsi"
        # Any additional provisioners such as NFS would be at the same level as "iscsi" (sibling of it)
        # IMPORTANT:
        #   total volume name (zvol/<datasetParentName>/<pvc name>) length cannot exceed 63 chars
        #   https://www.ixsystems.com/documentation/freenas/11.2-U5/storage.html#zfs-zvol-config-opts-tab
        #   standard volume naming overhead is 46 chars
        #   Which means names **MUST-BE** 17 characters or LESS!!!!
        datasets:
          parent_name: "main/k8s/iscsi/v"
          snapshot_ds_name: "main/k8s/iscsi/s"
        
        zvol:
          compression: "lz4"        # "" (inherit), lz4, gzip-9, etc
          blocksize: ""             # 512, 1K, 2K, 4K, 8K, 16K, 64K, 128K default is 16K
          enable_reservation: false

      name_prefix: csi-
      name_suffix: "-clustera"

      target_group:
        portal_group: 1             # get the correct ID from the "portal" section in the UI
        initiator_group: 1          # get the correct ID from the "initiators" section in the UI
        auth_type: "None"           # None, CHAP, or CHAP Mutual

        # get the correct ID from the "Authorized Access" section of the UI
        auth_group: ""              # only required if using CHAP

      extent:
        fs_type: "xfs"              # zvol block-based storage can be formatted as ext3, ext4, xfs
        block_size: 4096            # 512, 1024, 2048, or 4096
        rpm: "5400"                 # "" (let FreeNAS decide, currently defaults to SSD), Unknown, SSD, 5400, 7200, 10000, 15000
        avail_threshold: 0          # 0-100 (0 == ignore)

      # Once installed and validated, a test claim can be processed.
      test_claim:
        enabled: true               # true = attempt iscsi storage claim
        mode: "ReadWriteOnce"       # storage claim access mode
        size: "1Gi"                 # size of claim to request ("1Gi" is 1 Gibibytes)
        remove: true                # true = remove claim when test is completed (false leaves it alone)

    ###[ Democratic CSI NFS Settings ]#############################################################
    nfs:
      install_this: true            # Install the NFS provisioner

      provisioner: "freenas-nfs"
      namespace: "democratic-csi"
      release: "truenas-nfs"

      packages:
        - libnfs-utils

      storage_class:
        default_class: false
        reclaim_policy: "Delete"    # "Retain", "Recycle" or "Delete"
        volume_expansion: true

      zfs:
        sudo_enabled: true          # TrueNAS Core 12 requires non-root account have sudo access

        # Assumes pool named "main", dataset named "k8s", child dataset "nfs"
        # Any additional provisioners such as iSCSI would be at the same level as "nfs" (sibling of it)
        datasets:
          parent_name: "main/k8s/nfs/v"
          snapshot_ds_name: "main/k8s/nfs/s"        

          enable_quotas: true
          enable_reservation: false

          permissions:
            mode: '"0777"'
            user_id_num: 0          # 0 = root, needs User UID not a name (API needs a number)
            group_id_num: 0         # 0 = wheel, needs Group GUID not a name (API needs a number)

      # Once installed and validated, a test claim can be processed.
      test_claim:
        enabled: true               # true = attempt nfs storage claim
        mode: "ReadWriteOnce"       # storage claim access mode
        size: "1Gi"                 # size of claim to request ("1Gi" is 1 Gibibytes)
        remove: true                # true = remove claim when test is completed (false leaves it alone)

  ###[ Longhorn Installation Settings ]############################################################
  longhorn:
    install_this: true              # Install longhorn distributed cluster storage

    packages:
      - open-iscsi

    namespace: "longhorn-system"   # Add resources to this namespace
    release: "longhorn"            # Release value passed to Helm

    # Longhorn Dashboard
    dashboard:
      create_route: true           # Create Ingress Route to make accessible 
      enable_basic_auth: true      # Require Authentication to access dashboard
      path: "/longhorn"            # URI Path for Ingress Route

      # Encoded users and passwords for basic authentication
      allowed_users: "{{longhorn.dashboard_users}}"    

    # Define where ZFS Zvol should be created for Longhorn storage
    zfs:                           # Combined "rpool/longhorn"
      pool: "{{longhorn_zfs_pool|default('rpool')}}"
      volume_name: "longhorn"              

      zvol:
        options:
          volsize: "10G"
          compression: "lz4"       # "" (inherit), lz4, gzip-9, etc
          volblocksize: "16k"

        mountpoint: "/var/lib/longhorn"
        
    # The intent of longhorn is to be used instead of "local-path" storage class
    # once Longhorn is installed "local-path" will be disabled as the default storage class
    disable_local_path_as_default_storage_class: true

    # Once installed and validated, a test claim can be processed.
    test_claim:
      enabled: true               # true = attempt longhorn storage claim
      mode: "ReadWriteOnce"       # storage claim access mode
      size: "1Gi"                 # size of claim to request ("1Gi" is 1 Gibibytes)
      remove: true                # true = remove claim when test is completed (false leaves it alone)

  ###[ Prometheus Operator Installation Settings ]#################################################
  prometheus:
    install_this: true              # Install Prometheus Operator

    # Select release to use:  https://github.com/prometheus-operator/prometheus-operator/releases
    install_version: "v0.54.1"

    retention: "7d"                 # How long to retain data
    monitor_services:               # Services to monitor
      - kube-state-metrics
      - node-exporter
      - kubelet
      - traefik

    storage_claim:                  # Define where and how data is stored
      access_mode: "ReadWriteOnce"
      class_name: "freenas-iscsi-csi"
      claim_size: 20Gi

    node_exporter:
      # https://quay.io/repository/prometheus/node-exporter?tab=tags&tag=latest
      install_version: "1.3.1"
      image_name: "quay.io/prometheus/node-exporter"

    kube_state_metrics:
      # https://github.com/kubernetes/kube-state-metrics/releases
      install_version: "2.4.2"  #2.3.0
      image_name: "k8s.gcr.io/kube-state-metrics/kube-state-metrics"
      node_selector: "kubernetes.io/os: linux"    # Any linux node
      # node_selector: "node-type: worker"        # custom label example

    kube_rbac_proxy:
      # https://quay.io/repository/brancz/kube-rbac-proxy?tab=tags&tag=latest
      install_version: "0.8.0"
      image_name: "quay.io/brancz/kube-rbac-proxy"

  ###[ Grafana Installation Settings ]#############################################################
  grafana:
    install_this: true              # Install Grafana

    image_name: "grafana/grafana"
    install_version: "latest"

    storage_claim:                  # Define where and how data is stored
      access_mode: "ReadWriteOnce"
      class_name: "freenas-iscsi-csi"
      claim_size: 10Gi

    node_selector: "kubernetes.io/os: linux"    # Any linux node
    # node_selector: "node-type: worker"        # custom label example
