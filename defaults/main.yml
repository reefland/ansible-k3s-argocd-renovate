---
###[ Ansible Configuration Options ]#############################################################
# Do not log sensitive information / tokens, etc.  Maybe be handy to set to "false" during
# troubleshooting if you need to see ansible vault secret values.
no_log_secrets: true

install:
  ###[ Linux OS Configuration ]####################################################################
  os:
    non_root_user:                            # Some specifics about the non-root user
      name: "kube"
      shell: "/bin/bash"
      groups: "sudo"

    allow_passwordless_sudo: true             # Allow non-root use to passwordless sudo

    remove_packages:                             
      enabled: true
      packages:
        - "snapd"                             # Remove Snapd Demon, we don't need it.

    # Send all logs to a centralized logging server. This will be the 1st server defined
    # within the "k3s_control" group. All other cluster members will send logs to this system.
    # All logs from k3s hosts will be stored under: /var/log/central/<hostname>.log
    
    central_logging:                          
      log_path: "/var/log/central"            # Directory on central server to hold logs

  ### [ K3s Installation Settings ]################################################################
  k3s:
    # CLI_options are passed directly to install script "as-is", you can add to list.
    cli_options:
      # Do not start service after installation as it will have issues with ZFS
      - "INSTALL_K3S_SKIP_START=true"
      
      # This is to pin a specific version of k3s for initial installation
      # Select Release: https://github.com/k3s-io/k3s/releases
      - "INSTALL_K3S_VERSION={{k3s_install_version|default('v1.23.5+k3s1')}}"
      
      # Select installation channel to use (stable, latest, testing)
      # - "INSTALL_K3S_CHANNEL=latest"
    
    k3s_cli_var:
      - "--disable traefik"                   # Install a stable Traefik via helm instead
      - "--kube-apiserver-arg=feature-gates=MixedProtocolLBService=true"  # Allow Load Balancer to use TCP & UDP Ports

    # Define handy alias names for commands
    alias:
      enabled: true
      entries:
        - { alias_name: "k", command: "kubectl" }   # alias for kubectl  ($ k get all -A)

  ###[ Containerd Installation Settings ]##########################################################
  containerd:
    enabled: true

    # Hint to find the ZFS pool & dataset to create containerd mount point
    zfs:
      # detect_uuid will determine the UUID name used for the dataset name and include it.
      # ZFS on Root guide uses a random set of characters (UUID) in the naming convention
      # such as:  "rpool/ROOT/ubuntu_3wgs2q" where "3wgs2q" is the UUID to detect.
      
      # You can set to false and set your own or set uuid to empty string.

      # End result would be a dataset name such as:  rpool/ROOT/ubuntu_3wgs2q/var/lib/containerd
      detect_uuid: false
      pool: "rpool"
      dataset_prefix: "containerd"            # "ROOT/ubuntu"
      uuid: ""                                # "_"
      dataset_postfix: ""                     # "/var/lib/containerd"

  ###[ Longhorn Installation Settings ]############################################################
  longhorn:
    install_this: "{{longhorn_enabled|default(true)}}"  # Install longhorn distributed cluster storage

    # Select Release to use: https://github.com/longhorn/longhorn/releases 
    install_version: "{{longhorn_install_version|default('v1.2.4')}}"

    namespace: "longhorn-system"   # Add resources to this namespace
    release: "longhorn"            # Release value passed to Helm

    # Longhorn Dashboard
    dashboard:
      create_route: true           # Create Ingress Route to make accessible 
      enable_https: true           # Require HTTPS to access dashboard
      enable_basic_auth: true      # Require Authentication to access dashboard

      # Fully Qualified Domain for ingress routes - Traefik Load Balancer address name
      # This is the DNS name you plan to point to the Traefik ingress Load Balancer IP address.
      ingress_name: '{{k3s_cluster_ingress_name|default("k3s.{{ansible_domain}}")}}'

      # Default Dashboard URL:  https://k3s.{{ansible_domain}}/longhorn/
      path: "/longhorn"            # URI Path for Ingress Route

      # Encoded users and passwords for basic authentication
      allowed_users: "{{LONGHORN_DASHBOARD_USERS}}"

    # Define where ZFS Zvol should be created for Longhorn storage
    zfs:                           # Combined "rpool/longhorn"
      pool: "{{longhorn_zfs_pool|default('rpool')}}"
      volume_name: "longhorn"              

      zvol:
        options:
          volsize: "{{longhorn_vol_size|default('10G')}}"
          compression: "lz4"       # "" (inherit), lz4, gzip-9, etc
          volblocksize: "16k"

        mountpoint: "/var/lib/longhorn"
        
    # The intent of longhorn is to be used instead of "local-path" storage class
    # once Longhorn is installed "local-path" will be disabled as the default storage class
    disable_local_path_as_default_storage_class: true

    # Once installed and validated, a test claim can be processed.
    test_claim:
      enabled: true               # true = attempt longhorn storage claim
      mode: "ReadWriteOnce"       # storage claim access mode
      size: "1Mi"                 # size of claim to request ("1Mi" is 1 Mebibytes)
      remove: true                # true = remove claim when test is completed (false leaves it alone)

  ###[ Kube-VIP Installation Settings ]############################################################
  # Kube-VIP provides HA Load Balancer for the API service and optionally LoadBalancer services
  # It can optionally also provide Load Balancer services a pool of address to use instead of
  # using MetalLB.  When enabled this will disable k3s built-in Klipper Load Balancer.
  kube_vip:
    # When enabled, you must define variable "vip_endpoint_ip" at host or group level within 
    # inventory, host_var or group_var file. This must be set to an IP address. This IP address will
    # be a Load Balanced VIP cluster wide for the API service.  You can point kubectl to this IP 
    # address.
    enabled: true

    # Select release to use: https://github.com/kube-vip/kube-vip/releases
    install_version: "{{kube_vip_install_version|default('v0.4.2')}}"

    # To use Kube-VIP Cloud Provider as LoadBalancer instead of MetalLB
    lb:
      # When enabled, this will disable k3s built-in Klipper Load Balancer and enable kube-vip LB
      # instead.

      # When enabled, you must define variable "vip_lb_ip_range" at host or group level within 
      # inventory, host_var or group_var file. This must be set to an IP range or CIDR range.
      # This will define the pool of IP addresses to hand out to serviced of type LoadBalancer.
      enabled: true

  ###[ MetalLB Installation Settings ]#############################################################
  # When enabled this will disable k3s built-in Klipper Load Balancer and enable MetalLB instead.
  # This can provide Load Balancer services a pool of addresses to use
  metallb:
    enabled: false

    # Select release to use:  https://github.com/metallb/metallb/releases
    # Alternate: https://metallb.universe.tf/installation/
    install_version: "{{metallb_install_version|default('v0.12.1')}}"

  ###[ Cert Manager Installation Settings ]########################################################
  cert_manager:
    # Select release to use:  https://github.com/cert-manager/cert-manager/releases
    install_version: "{{cert_manager_install_version|default('v1.7.1')}}"

    namespace: "cert-manager"

    # to create prod certificates --extra-vars "le_staging=false"
    le_staging: "{{le_staging|default(true)}}"

    # List of Domain Names for LetsEncrypt Certificates stored in letsencrypt_secrets.yml
    domains: "{{LE_DOMAINS}}"

  ###[ Traefik Installation Settings ]#############################################################
  traefik:

    # Select release to use: https://github.com/traefik/traefik/releases
    install_version: "{{traefik_install_version|default('v2.6.3')}}"

    namespace: "traefik"                      # Add resources to this namespace
    release: "traefik"                        # Release value passed to Helm

    # Traefik Dashboard
    dashboard:
      create_route: true                      # Create Ingress Router to make accessible 
      enable_https: true                      # Require HTTPS to access dashboard
      enable_basic_auth: true                 # Require Authentication to access dashboard

      # Fully Qualified Domain for ingress routes - Traefik Load Balancer address name
      # This is the DNS name you plan to point to the Traefik ingress Load Balancer IP address.
      ingress_name: '{{k3s_cluster_ingress_name|default("k3s.{{ansible_domain}}")}}'
      
      # Default Dashboard URL:  https://k3s.{{ansible_domain}}/dashboard/
      path: "/dashboard"                      # PathPrefix for dashboard

      # Encoded users and passwords for basic authentication stored in k3s_traefik_api_secrets.yml
      allowed_users: "{{TRAEFIK_DASHBOARD_USERS}}"    

  ###[ Democratic CSI Installation Settings ]######################################################
  democratic_csi:
    truenas:
      # See file "truenas_api_secrets.yml" to set secrets (and be sure to use ansible vault to encrypt!)
      http_connection:
        protocol: "https"
        port: 443
        allow_insecure: false
        host: "{{democratic_csi.http_hostname}}"
        api_key: "{{democratic_csi.http_api_key}}"

      ssh_connection:
        host: "{{democratic_csi.ssh_hostname}}"
        port: 22
        user: "{{democratic_csi.ssh_username}}"
        
        # Must use a password or private key
        #password: "{{democratic_csi.ssh_password}}"
        private_key: "{{democratic_csi.ssh_private_key}}"

      iscsi_connection:
        host: "{{democratic_csi.iscsi_hostname}}"
        port: 3260
        #interface: ""                 # leave empty to omit usage of -I with iscsiadm

      nfs_connection:
        host: "{{democratic_csi.nfs_hostname}}"

    ###[ Democratic CSI iSCSI Settings ]###########################################################
    # Available: https://github.com/democratic-csi/democratic-csi/tree/master/examples
    iscsi:
      install_this: true            # Install the iSCSI provisioner

      provisioner: "freenas-iscsi"
      namespace: "democratic-csi"
      release: "truenas-iscsi"

      storage_class:
        default_class: false
        reclaim_policy: "Delete"    # "Retain", "Recycle" or "Delete"
        volume_expansion: true

      zfs:
        # Assumes pool named "main", dataset named "k8s", child dataset "iscsi"
        # Any additional provisioners such as NFS would be at the same level as "iscsi" (sibling of it)
        # IMPORTANT:
        #   total volume name (zvol/<datasetParentName>/<pvc name>) length cannot exceed 63 chars
        #   https://www.ixsystems.com/documentation/freenas/11.2-U5/storage.html#zfs-zvol-config-opts-tab
        #   standard volume naming overhead is 46 chars
        #   Which means names **MUST-BE** 17 characters or LESS!!!!
        datasets:
          parent_name: "main/k8s/iscsi/v"
          snapshot_ds_name: "main/k8s/iscsi/s"
        
        zvol:
          compression: "lz4"        # "" (inherit), lz4, gzip-9, etc
          blocksize: ""             # 512, 1K, 2K, 4K, 8K, 16K, 64K, 128K default is 16K
          enable_reservation: false

      name_prefix: csi-
      name_suffix: "-clustera"

      target_group:
        portal_group: 1             # get the correct ID from the "portal" section in the UI
        initiator_group: 1          # get the correct ID from the "initiators" section in the UI
        auth_type: "None"           # None, CHAP, or CHAP Mutual

        # get the correct ID from the "Authorized Access" section of the UI
        auth_group: ""              # only required if using CHAP

      extent:
        fs_type: "xfs"              # zvol block-based storage can be formatted as ext3, ext4, xfs
        block_size: 4096            # 512, 1024, 2048, or 4096
        rpm: "5400"                 # "" (let FreeNAS decide, currently defaults to SSD), Unknown, SSD, 5400, 7200, 10000, 15000
        avail_threshold: 0          # 0-100 (0 == ignore)

      # Once installed and validated, a test claim can be processed.
      test_claim:
        enabled: true               # true = attempt iscsi storage claim
        mode: "ReadWriteOnce"       # storage claim access mode
        size: "1Mi"                 # size of claim to request ("1Mi" is 1 Mebibytes)
        remove: true                # true = remove claim when test is completed (false leaves it alone)

    ###[ Democratic CSI NFS Settings ]#############################################################
    nfs:
      install_this: true            # Install the NFS provisioner

      provisioner: "freenas-nfs"
      namespace: "democratic-csi"
      release: "truenas-nfs"

      storage_class:
        default_class: false
        reclaim_policy: "Delete"    # "Retain", "Recycle" or "Delete"
        volume_expansion: true

      zfs:
        sudo_enabled: true          # TrueNAS Core 12 requires non-root account have sudo access

        # Assumes pool named "main", dataset named "k8s", child dataset "nfs"
        # Any additional provisioners such as iSCSI would be at the same level as "nfs" (sibling of it)
        datasets:
          parent_name: "main/k8s/nfs/v"
          snapshot_ds_name: "main/k8s/nfs/s"        

          enable_quotas: true
          enable_reservation: false

          permissions:
            mode: '"0777"'
            user_id_num: 0          # 0 = root, needs User UID not a name (API needs a number)
            group_id_num: 0         # 0 = wheel, needs Group GUID not a name (API needs a number)

      # Once installed and validated, a test claim can be processed.
      test_claim:
        enabled: true               # true = attempt nfs storage claim
        mode: "ReadWriteOnce"       # storage claim access mode
        size: "1Mi"                 # size of claim to request ("1Mi" is 1 Mebibytes)
        remove: true                # true = remove claim when test is completed (false leaves it alone)

  ###[ Prometheus Operator Installation Settings ]#################################################
  prometheus_operator:
    install_this: true            # Install Prometheus Operator

    # Select release to use:  https://github.com/prometheus-community/helm-charts/releases
    install_version: "{{prometheus_op_install_version|default('34.7.1')}}"

    namespace: "monitoring"
    release: "kube-stack-prometheus"

    prometheus:
      retention: "14d"             # How long to retain data

      storage_claim:              # Define where and how data is stored
        access_mode: "ReadWriteOnce"
        class_name: "freenas-iscsi-csi"
        claim_size: 25Gi

      # Prometheus Web Interface
      dashboard:
        create_route: true        # Create Ingress Route to make accessible 
        enable_basic_auth: true   # Require Authentication to access dashboard

        # Fully Qualified Domain for ingress routes - Traefik Load Balancer address name
        # This is the DNS name you plan to point to the Traefik ingress Load Balancer IP address.
        ingress_name: '{{k3s_cluster_ingress_name|default("k3s.{{ansible_domain}}")}}'

        # Default Dashboard URL:  https://k3s.{{ansible_domain}}/prometheus/
        path: "/prometheus"       # URI Path for Ingress Route

        # Encoded users and passwords for basic authentication
        allowed_users: "{{prometheus_operator.prometheus.dashboard_users}}"

    grafana:
      storage_claim:              # Define where and how data is stored
        access_mode: "ReadWriteOnce"
        class_name: "freenas-iscsi-csi"
        claim_size: 5Gi

      # Grafana Dashboard
      dashboard:
        create_route: true        # Create Ingress Route to make accessible 
        enable_basic_auth: false  # Has its own login page

        # Fully Qualified Domain for ingress routes - Traefik Load Balancer address name
        # This is the DNS name you plan to point to the Traefik ingress Load Balancer IP address.
        ingress_name: '{{k3s_cluster_ingress_name|default("k3s.{{ansible_domain}}")}}'

        # Default Dashboard URL:  https://k3s.{{ansible_domain}}/grafana/
        path: "/grafana"          # URI Path for Ingress Route

    alertmanager:
      storage_claim:              # Define where and how data is stored
        access_mode: "ReadWriteOnce"
        class_name: "freenas-iscsi-csi"
        claim_size: 3Gi

      # Alertmanager Web Interface
      dashboard:
        create_route: true        # Create Ingress Route to make accessible 
        enable_basic_auth: true   # Has its own login page

        # Fully Qualified Domain for ingress routes - Traefik Load Balancer address name
        # This is the DNS name you plan to point to the Traefik ingress Load Balancer IP address.
        ingress_name: '{{k3s_cluster_ingress_name|default("k3s.{{ansible_domain}}")}}'

        # Default Dashboard URL:  https://k3s.{{ansible_domain}}/alertmanager/
        path: "/alertmanager"     # URI Path for Ingress Route

        # Encoded users and passwords for basic authentication
        allowed_users: "{{prometheus_operator.alertmanager.dashboard_users}}"
      
      # Configuration Values for Helm for Alert Configurations
      # Reference: https://prometheus.io/docs/alerting/latest/configuration/
      helm_config_values: |
        config:
          global:
            resolve_timeout: 5m
          route:
            receiver: default-receiver
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 4h
            group_by: [cluster, alertname]
            # All alerts that do not match the following child routes
            # will remain at the root node and be dispatched to 'default-receiver'.
            routes:
            #  - matchers:
            #    - service=~"foo1|foo2|baz"
            #  receiver: database-team
          receivers:
          - name: 'default-receiver'
            # https://prometheus.io/docs/alerting/latest/configuration/#slack_config
            slack_configs:
              - api_url: "{{vault_slack_config_api_url}}"
                send_resolved: true
                channel: 'monitoring'
                {% raw %}text: "{{ range .Alerts }}<!channel> {{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"{% endraw %}

