---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  namespace: {{install.argocd.namespace}}
  name: {{prometheus_op.release_name}}
  # cascade deletes
  finalizers:
    - resources-finalizer.argocd.argoproj.io
  annotations:
    argocd.argoproj.io/sync-wave: "14"  # Lower the number the sooner to deploy
    
spec:
  project: {{install.prometheus_operator.argocd_project|default("default")}}
  revisionHistoryLimit: {{install.argocd.revision_History_Limit|default(3)}}

  source:
    chart: kube-prometheus-stack
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: {{install.prometheus_operator.install_version}}
    helm:
      skipCrds: true
      # Reference: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
      values: |
        ## Provide a name to substitute for the full names of resources
        fullnameOverride: "prometheus"

        global:
          rbac:
            pspEnabled: false

        kubeApiServer:
          enabled: true

        kubelet:
          enabled: true
          namespace: kube-system
          serviceMonitor:
            cAdvisor: false
            ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            cAdvisorMetricRelabelings:
              # Drop less useful container CPU metrics.
              - sourceLabels: [__name__]
                action: drop
                regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
              # Drop less useful container / always zero filesystem metrics.
              - sourceLabels: [__name__]
                action: drop
                regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
              # Drop less useful / always zero container memory metrics.
              - sourceLabels: [__name__]
                action: drop
                regex: 'container_memory_(mapped_file|swap)'
              # Drop less useful container process metrics.
              - sourceLabels: [__name__]
                action: drop
                regex: 'container_(file_descriptors|tasks_state|threads_max)'
              # Drop container spec metrics that overlap with kube-state-metrics.
              - sourceLabels: [__name__]
                action: drop
                regex: 'container_spec.*'
              # Drop cgroup metrics with no pod - Breaks networking metrics
              #- sourceLabels: [id, pod]
              #  action: drop
              #  regex: '.+;'
              # Change instance IP to node name
              - action: replace
                sourceLabels:
                  - node
                targetLabel: instance

        kubeControllerManager:
          # Enable when more than one Control Plane node is available
          enabled: {{ (groups['k3s_control'] | length > 1) | ternary('true','false') }}
          endpoints:
{% for IP in controlPlaneIPs.stdout_lines[0] | split(' ') %}
            - {{IP}}
{% endfor %}
          service:
            enabled: true
            port: 10257
            targetPort: 10257
          serviceMonitor:
            enabled: true
            https: true
            insecureSkipVerify: true

        coreDns:
          enabled: true

        kubeScheduler:
          # Enable when more than one Control Plane node is available
          enabled: {{ (groups['k3s_control'] | length > 1) | ternary('true','false') }}
          endpoints:
{% for IP in controlPlaneIPs.stdout_lines[0] | split(' ') %}
            - {{IP}}
{% endfor %}
          service:
            enabled: true
            port: 10259
            targetPort: 10259
          serviceMonitor:
            enabled: true
            https: true
            insecureSkipVerify: true

        kubeProxy:
          # Enable when more than one Control Plane node is available
          enabled: {{ (groups['k3s_control'] | length > 1) | ternary('true','false') }}
          endpoints:
{% for IP in controlPlaneIPs.stdout_lines[0] | split(' ') %}
            - {{IP}}
{% endfor %}

        kubeEtcd:
          # Enable when more than one Control Plane node is available
          enabled: {{ (groups['k3s_control'] | length > 1) | ternary('true','false') }}
          endpoints:
{% for IP in controlPlaneIPs.stdout_lines[0] | split(' ') %}
            - {{IP}}
{% endfor %}
          service:
            enabled: true
            port: 2381
            targetPort: 2381

        kubeStateMetrics:
          enabled: true

        kube-state-metrics:
          fullnameOverride: kube-state-metrics
          selfMonitor:
            enabled: true
          podSecurityPolicy:
            enabled: false
          prometheus:
            monitor:
              enabled: true
              relabelings:
                - action: replace
                  regex: (.*)
                  replacement: $1
                  sourceLabels:
                    - __meta_kubernetes_pod_node_name
                  targetLabel: instance

        nodeExporter:
          enabled: true
        
        prometheus-node-exporter:
          fullnameOverride: node-exporter
          podLabels:
            jobLabel: node-exporter
          extraArgs:
            - --collector.filesystem.mount-points-exclude=^/(dev|proc|run|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
            - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          service:
            portName: http-metrics
          rbac:
            pspEnabled: false
          prometheus:
            monitor:
              enabled: true
              relabelings:
                - action: replace
                  regex: (.*)
                  replacement: $1
                  sourceLabels:
                    - __meta_kubernetes_pod_node_name
                  targetLabel: instance

        ## Manages Prometheus and Alertmanager components
        prometheusOperator:
          enabled: true

        # Prometheus values
        prometheus:
          enabled: true
          prometheusSpec:
            # Prometheus StorageSpec for persistent data
            # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: {{install.prometheus_operator.prometheus.storage_claim.class_name}}
                  accessModes: 
                    - {{install.prometheus_operator.prometheus.storage_claim.access_mode}}
                  resources:
                    requests:
                      storage: {{install.prometheus_operator.prometheus.storage_claim.claim_size}}
            # Allow finding resources in other namespaces
            ruleSelectorNilUsesHelmValues: false
            serviceMonitorSelectorNilUsesHelmValues: false
            podMonitorSelectorNilUsesHelmValues: false
            probeSelectorNilUsesHelmValues: false

            # whichever retention hits first triggers cleanup
            retention: {{install.prometheus_operator.prometheus.retention}}
            retentionSize: {{install.prometheus_operator.prometheus.retention_size}}
{% if install.prometheus_operator.prometheus.dashboard.path is defined %}
            externalUrl: {{install.prometheus_operator.prometheus.dashboard.path}}
{% endif %}
            # Additional Scrape Jobs go inside this ConfigSecret if needed
            additionalScrapeConfigsSecret:
              enabled: false
              name: additional-scrape-configs
              key: prometheus-additional-scrape-configs.yaml

            additionalScrapeConfigs:
              # CADVISOR SCRAPE JOB for externally installed cadvisor because of k8s with containerd problems
              - job_name: "kubernetes-cadvisor"
                kubernetes_sd_configs:
                  - role: pod  # we get needed info from the pods
                    namespaces:
                      names: 
                        - cadvisor
                    selectors:
                      - role: pod
                        label: "app=cadvisor"  # and only select the cadvisor pods with this label set as source
                metric_relabel_configs:  # we relabel some labels inside the scraped metrics
                  # this should look at the scraped metric and replace/add the label inside
                  - source_labels: [container_label_io_kubernetes_pod_namespace]
                    target_label: "namespace"
                  - source_labels: [container_label_io_kubernetes_pod_name]
                    target_label: "pod"
                  - source_labels: [container_label_io_kubernetes_container_name]
                    target_label: "container"

                  # These are also applied to native kublet cadvisor to drop uesless metrics by Kube-Prometheus-Stack
                  # Drop less useful container CPU metrics.
                  - source_labels: [__name__]
                    action: drop
                    regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
                  # Drop less useful container / always zero filesystem metrics.
                  - source_labels: [__name__]
                    action: drop
                    regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
                  # Drop less useful / always zero container memory metrics.
                  - source_labels: [__name__]
                    action: drop
                    regex: 'container_memory_(mapped_file|swap)'
                  # Drop less useful container process metrics.
                  - source_labels: [__name__]
                    action: drop
                    regex: 'container_(file_descriptors|tasks_state|threads_max)'
                  # Drop container spec metrics that overlap with kube-state-metrics.
                  - source_labels: [__name__]
                    action: drop
                    regex: 'container_spec.*'

                  # Change instance IP to node name
                  - action: replace
                    source_labels: [node]
                    target_label: "instance"

                # metrics_path is required to match upstream rules and charts
                relabel_configs:
                  - action: replace
                    source_labels: [__metrics_path__]
                    target_label: metrics_path

{% if install.prometheus_operator.exporters.zfs_exporter.enabled|default(false)|bool == true%}
              # ZFS Exporter Scrape Job
              - job_name: "zfs-exporter"
                static_configs:
                  - targets:
                    {% filter indent(width=10) %}{% for IP in controlPlaneIPs.stdout_lines[0] | split(' ') %}- "{{ IP | replace("'","") }}:{{ prometheus_op.zfs_exporter.port }}"
{% endfor %}          {% if (workerNodeIPs.stdout_lines[0] is defined) and (workerNodeIPs.stdout_lines[0] | replace("'","")| length > 0) %}{% for IP in workerNodeIPs.stdout_lines[0] | split(' ') %}- "{{ IP | replace("'","") }}:{{ prometheus_op.zfs_exporter.port }}"
{% endfor %}{% endif %}{% endfilter %}
{{''}}
                # Drop ZFS metrics unlikely to be needed
                metric_relabel_configs:
                  - source_labels: [__name__]
                    action: drop
                    regex: 'zfs_dataset_(quota_bytes|logical_used_bytes)'
                  - source_labels: [__name__]
                    action: drop
                    regex: 'zfs_pool_(deduplication_ratio)'
{% endif %}

        # Graphana values
        # Reference: https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
        grafana:
          fullnameOverride: "grafana"
          enabled: true
          rbac:
            pspEnabled: false

          grafana.ini:
            server:
              root_url: https://{{install.prometheus_operator.grafana.dashboard.ingress_name|default("k3s.{{ansible_domain}}")|lower}}{{install.prometheus_operator.grafana.dashboard.path}}/
            security:
              allow_embedding: true
            auth:
              login_cookie_name: grafana_session_k3s
              login_maximum_inactive_lifetime_duration: 2M
              login_maximum_lifetime_duration: 2M
              token_rotation_interval_minutes: 1000

          persistence:
            type: pvc
            enabled: true
            storageClassName: {{install.prometheus_operator.grafana.storage_claim.class_name}}
            accessModes: 
              - {{install.prometheus_operator.grafana.storage_claim.access_mode}}
            size: {{install.prometheus_operator.grafana.storage_claim.claim_size}}
            # annotations: {}
            finalizers:
              - kubernetes.io/pvc-protection
            # subPath: ""
            # existingClaim:
          admin:
            existingSecret: "{{prometheus_op.release_name}}-grafana"
            userKey: admin-user
            passwordKey: admin-password

          plugins:
            - grafana-piechart-panel
            - grafana-clock-panel

          sidecar:
            dashboards:
              # To enable sidecar
              enabled: true
              # Label key that configMaps should have in order to be mounted 
              label: {{install.prometheus_operator.grafana.sidecar.label}}
              # value of label that the configmaps with dashboards are set to
              labelValue: {{install.prometheus_operator.grafana.sidecar.label_value}}
              # Folder where the configMaps are mounted in Grafana container
              folder: /tmp/dashboards
              # To enable searching configMap accross all namespaces
              searchNamespace: {{install.prometheus_operator.grafana.sidecar.search_namespaces}}

        # AlertManager values
        # Reference: https://prometheus.io/docs/alerting/latest/configuration/
        alertmanager:
          fullnameOverride: "alertmanager"
          enabled: true

          # Reference: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
          alertmanagerSpec:
            retention: 120h
            
            # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: {{install.prometheus_operator.alertmanager.storage_claim.class_name}}
                  accessModes: 
                    - {{install.prometheus_operator.alertmanager.storage_claim.access_mode}}
                  resources:
                    requests:
                      storage: {{install.prometheus_operator.alertmanager.storage_claim.claim_size}}
            externalUrl: {% if install.traefik.dashboard.enable_https|default(false)|bool == false %}http://{% else %}https://{% endif %}{{install.prometheus_operator.alertmanager.dashboard.ingress_name|default("k3s.{{ansible_domain}}")|lower}}{{install.prometheus_operator.alertmanager.dashboard.path|default("/alertmanager")}}

            useExistingSecret: true
            configSecret: alertmanager-{{prometheus_op.release_name}}-alertmanager-config

  destination:
    server: https://kubernetes.default.svc
    namespace: {{install.prometheus_operator.namespace}}

  # Allow changes to number of replicas (down to zero)
  ignoreDifferences:
    - group: apps
      kind: StatefulSet
      jsonPointers:
        - /spec/replicas

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - Validate=true
